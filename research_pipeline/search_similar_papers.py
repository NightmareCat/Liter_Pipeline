# search_similar_papers.py

import os
import json
import numpy as np
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Tuple
from pipeline.get_embedding_bgem3 import get_embedding_bge_m3

load_dotenv()
Embedding_Model_select = 2 # 1-qwen embedding3  2- BGE-M3

client = OpenAI(
    api_key=os.getenv("QWEN_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

def get_query_embedding(query: str) -> np.ndarray:
    response = client.embeddings.create(
        input=query,
        model="text-embedding-v3"
    )
    return np.array(response.data[0].embedding)

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def load_all_embeddings(folder: Path) -> List[Tuple[str, str, List[Tuple[str, np.ndarray]]]]:
    """
    åŠ è½½æ‰€æœ‰è®ºæ–‡çš„ embedding
    è¿”å› (æ–‡ä»¶å, é¢„è§ˆæ‘˜è¦, å‘é‡åˆ—è¡¨[(æ®µè½æ–‡æœ¬, å‘é‡)]) çš„åˆ—è¡¨
    """
    results = []
    for file in folder.glob("*.json"):
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        if "embeddings" in data:
            # å¤šæ®µåµŒå…¥æ ¼å¼
            vec_list = [
                (item["text"], np.array(item["embedding"]))
                for item in data["embeddings"]
            ]
        elif "embedding" in data:
            # å‘åå…¼å®¹å•æ®µæ ¼å¼
            vec_list = [(data.get("text", ""), np.array(data["embedding"]))]
        else:
            continue  # éæ³•æ•°æ®ï¼Œè·³è¿‡

        preview = data.get("text", "")[:200].replace("\n", " ").strip()
        results.append((file.stem, preview, vec_list))

    return results

def search_similar(query: str, data_folder: str, top_k: int = 5) -> List[dict]:
    """
    å¯¹ç»™å®šæŸ¥è¯¢è¿›è¡ŒåŒ¹é…ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœã€‚
    æ¯ä¸ªç»“æœåŒ…å«ï¼šè®ºæ–‡åã€ç›¸ä¼¼åº¦ã€æœ€ç›¸å…³æ®µè½ã€‚
    """
    query_vec = get_query_embedding(query)
    if Embedding_Model_select == 1:
        query_vec = get_query_embedding(query)
    elif Embedding_Model_select == 2:
        embedding_tensor = get_embedding_bge_m3(query)
        query_vec = embedding_tensor.cpu().tolist()
    
    papers = load_all_embeddings(Path(data_folder))

    scored = []
    for name, preview, vec_list in papers:
        best_score = -1.0
        best_text = ""
        for para_text, vec in vec_list:
            score = cosine_similarity(query_vec, vec)
            if score > best_score:
                best_score = score
                best_text = para_text
        scored.append({
            "document": name,
            "similarity": round(float(best_score), 4),
            "best_paragraph": best_text
        })

    # æ’åºåè¿”å› top_k ä¸ªç»“æœ
    scored.sort(key=lambda x: x["similarity"], reverse=True)
    return scored[:top_k]


if __name__ == "__main__":
    query = "æå‡å¼±åœºä¸‹å«æ˜Ÿçš„å¹²æ‰°æŠ‘åˆ¶èƒ½åŠ›"
    results = search_similar(query, "embedding_qwen_long", top_k=8)

    print(f"\nğŸ” æ­£åœ¨åŒ¹é…è¯¾é¢˜æ–¹å‘ï¼š{query}\n")
    print(f"ğŸ“„ Top {len(results)} æœ€ç›¸å…³è®ºæ–‡ï¼š\n")
    for idx, item in enumerate(results, 1):
        print(f"{idx}. ğŸ“ {item['document']}  (åŒ¹é…åº¦: {item['similarity']:.4f})")
        print(f"- æœ€ç›¸å…³æ®µè½:\n\n  {item['best_paragraph'][:180]}...\n")
